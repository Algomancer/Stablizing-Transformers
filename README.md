# Stablizing Transformers

This project offers an open-source implimentation of [ÏƒReparam](https://arxiv.org/pdf/2303.06296.pdf), a technique designed to enhance Transformer training stability by addressing attention entropy collapse. The approach reparametrizes linear layers with spectral normalization and a learned scalar, showcasing improved robustness across various tasks without the need for specific hyperparameters without warmup and adaptive optimizers.

ÏƒReparam + pre-LN ðŸ’¯
